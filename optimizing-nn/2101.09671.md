# Pruning and Quantization for Deep Neural Network Acceleration: A Survey #

## Sections
1. Idea
2. Introduction
3. Convolutional Neural Networks
4. 

## 1. Idea
Complex network architectures require significant computational resources and power. They
are difficult to deploy in time and compute constrained settings. Network compression 
can optimize these networks, with little loss of accuracy. This paper surveys two
network compression techniques (primarily with respect to CNNs): Quantization and Pruning. **Static pruning is done offline** and **dynamic pruning is performed at run-time**. Tradeoffs between element-wise to network-wise pruning are discussed in the paper. Quantization optimizes network by reducing the precision of datatypes, by quantizing them typically to 8-bit integers. Both techniques can be used independently or combined. The paper also compares different techniques, compressed network accuracy results obtained by use of different frameworks, and provide guidance for compressing networks.

## 2. Introduction
Neural networks have demonstrated human-level capabilities but at the cost of significant computational complexity and training time. Such large networks are difficult to deploy in embedded environments. Over-parameterization is the property of neural networks where redundant neurons don't improve accuracy of results. This redundancy can often be removed with little to no accuracy loss.

Over-parameterization can be alleviated by: 
1. Network Structure - Novel Components, Network Architecture Search (NAS), Knowledge Distillation (KD)
2. Network Optimization - Quantization and Pruning
3. Hardware Accelerator Design - FPGAs, ASIC, GPU/TPU/CPU 
 
Network Structure
1. Novel components - Separable Convolution in MobileNets, Inception Blocks, ResNets, DenseNets, dropout.
2. NAS - Programmatically search for a highly efficient network structure in a large predefined search space. Time-consuming but final architecture outperforms manually designed nets.
3. KD - Teacher and Student Networks. Student network is less complex. Use soft targets from teacher to train student.

Network Optimization
1. Computational Convolution Optimization - FFT Based Convolution, Im2Col, Winograd Convolution
2. Parameter Factorization - Convert high-rank tensors to low-rank. Reduces number of computations and compresses models. Applied to both CNN and fully connected layers.
3. Pruning and Quantization

Hardware Accelerators
1. CPUs optimized with specialized AI instructions within specialized SIMD units.
2. GPUs - Specialized tensor units
3. ASICs - Applications include security cameras or mobile devices. GPT, ARM, NVidia, etc.
   Used at datacenters for both training and inference.
4. TPU, Habana (Intel), etc.
5. FPGA - Preferred over ASICs due to long design cycles. FPGAs also provide NN libraries and specialized NN chips.

## Convolutional Neural Networks
Paper introduces Convolutional Neural Networks. It discusses GeMM library in brief, which uses im2col to does CNN matrix-multiplies efficiently.

### Depth-wise Separable Convolutions
Mobilenet uses depth-wise separable convolutions. These convolutions act on per channel. If the image is 3 channel (RGB) and kernel is 2 x 2, and output channels is 4, then general convolution causes 2 x 2 x 3 x 4 = 48 operations.
Depth-wise seperable convolution causes 2 x 2 x 3 (1 per input channel) = 12 and 1 x 1 x 3 x 4 = 12 point-wise convolution. Therefore, 24 operations.

### Inception, ResNets, DenseNets
